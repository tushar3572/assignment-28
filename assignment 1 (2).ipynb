{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dadddf-aa19-48ef-99e5-c98d25c9af01",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Answer : 1\n",
    "    \n",
    "Linear SVM Regression: Primal Formula\n",
    "Suppose we have a set of training data where xn is a multivariate set of N observations with observed response\n",
    "values yn. subject to all residuals having a value less than ε; or, in equation form: ∀ n : | y n − ( x n ′ β + b ) | ≤ ε .    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701df24a-a820-4b5e-809d-477006c82645",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Answer : 2\n",
    "\n",
    "Apparently, the objective function is the geometric margin of the hyperplane (w, b). \n",
    "The constraints represent the fact that the objective function is the minimum of the set of geometric margins of \n",
    "the hyperplane (w, b) w.r.t. all the training examples.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce84b7e4-32f5-4f9c-bf2b-c37b42b5398d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Answer : 3\n",
    "    \n",
    "SVM has a technique called the kernel trick. These are functions that take low dimensional input space and \n",
    "transform it into a higher-dimensional space i.e. it converts not separable problem to separable problem.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b226957a-df8a-411f-8e57-d5577002c52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Answer : 4\n",
    "    \n",
    "Support vectors are data points that are closer to the hyperplane and influence the position and\n",
    "orientation of the hyperplane. Using these support vectors, we maximize the margin of the classifier.\n",
    "Deleting the support vectors will change the position of the hyperplane. \n",
    "These are the points that help us build our SVM.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a1b633-37dd-406a-a4b0-5f0d4127b4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Answer : 5\n",
    "    \n",
    "1. Hyperplane:\n",
    "The hyperplane is the decision boundary that separates the data points into different classes. In a binary classification problem, it's a line in 2D or a plane in higher dimensions. Here's an example in 2D:\n",
    "\n",
    "\n",
    "In this case, the hyperplane is the line that separates the blue circles from the red crosses. The equation of the hyperplane is \n",
    "𝑤0 + w1x1 + w2x2 = 0, where w0,w1 and w2 are parameters of the hyperplane.\n",
    "\n",
    "2. Margin:\n",
    "The margin is the distance between the hyperplane and the nearest data point from either class. In a hard margin SVM, the margin is maximized, meaning the hyperplane is positioned as far away from the nearest data points as possible. In a soft margin SVM, some points may fall within the margin or even on the wrong side of the margin due to noise or overlapping classes.\n",
    "\n",
    "3. Hard Margin SVM:\n",
    "In a hard margin SVM, the data is assumed to be linearly separable without any errors. Here's an example:\n",
    "\n",
    "\n",
    "The solid line represents the hyperplane, and the dashed lines represent the margin. All data points are correctly classified, and there's no violation of the margin.\n",
    "\n",
    "4. Soft Margin SVM:\n",
    "In a soft margin SVM, the margin is allowed to be violated by some points to allow for better generalization. Here's an example:\n",
    "\n",
    "\n",
    "In this case, the margin is violated by one data point (the red cross), but overall the hyperplane still separates the majority of the points correctly.\n",
    "\n",
    "Marginal Plane:\n",
    "The marginal plane is the plane parallel to the hyperplane and equidistant from it by a margin. It helps define the margin and support vectors.\n",
    "\n",
    "These illustrations demonstrate how SVM works with different types of margins and hyperplanes to classify data points.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e80bec-61d9-4fa3-9d17-e2021212ff0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "   #Answer : 6\n",
    "    \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # Using only two features for visualization\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# Define a linear SVM classifier from scratch\n",
    "class LinearSVM:\n",
    "    def __init__(self, learning_rate=0.001, epochs=1000, C=1):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.C = C\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        for _ in range(self.epochs):\n",
    "            for i, x in enumerate(X):\n",
    "                condition = y[i] * (np.dot(x, self.weights) - self.bias) >= 1\n",
    "                if condition:\n",
    "                    self.weights -= self.learning_rate * (2 * self.C * self.weights)\n",
    "                else:\n",
    "                    self.weights -= self.learning_rate * (2 * self.C * self.weights - np.dot(x, y[i]))\n",
    "                    self.bias -= self.learning_rate * y[i]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.sign(np.dot(X, self.weights) - self.bias)\n",
    "\n",
    "# Train the linear SVM classifier from scratch\n",
    "svm_scratch = LinearSVM(C=1)\n",
    "svm_scratch.fit(X_train_std, y_train)\n",
    "\n",
    "# Predict labels for the testing set\n",
    "y_pred_scratch = svm_scratch.predict(X_test_std)\n",
    "\n",
    "# Compute the accuracy of the model\n",
    "accuracy_scratch = accuracy_score(y_test, y_pred_scratch)\n",
    "print(\"Accuracy of linear SVM classifier from scratch:\", accuracy_scratch)\n",
    "\n",
    "# Plot the decision boundaries of the trained model\n",
    "def plot_decision_boundary(X, y, classifier, title):\n",
    "    h = 0.02  # Step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')\n",
    "    plt.xlabel('Sepal length (standardized)')\n",
    "    plt.ylabel('Sepal width (standardized)')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Plot decision boundary of the linear SVM classifier from scratch\n",
    "plot_decision_boundary(X_train_std, y_train, svm_scratch, \"Decision Boundary (From Scratch)\")\n",
    "\n",
    "# Compare with scikit-learn implementation\n",
    "svm_sklearn = SVC(kernel='linear', C=1)\n",
    "svm_sklearn.fit(X_train_std, y_train)\n",
    "y_pred_sklearn = svm_sklearn.predict(X_test_std)\n",
    "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
    "print(\"Accuracy of linear SVM classifier from scikit-learn:\", accuracy_sklearn)\n",
    "\n",
    "# Plot decision boundary of the linear SVM classifier from scikit-learn\n",
    "plot_decision_boundary(X_train_std, y_train, svm_sklearn, \"Decision Boundary (Scikit-learn)\")\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
